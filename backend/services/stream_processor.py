"""ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚µãƒ¼ãƒ“ã‚¹"""
from typing import AsyncGenerator
import json
import re
import traceback
from datetime import datetime
from agno.agent import Agent
from agno.tools.reasoning import ReasoningTools

from models.enums import AgentServiceType
from config.logging_config import get_logger

logger = get_logger(__name__)

async def stream_with_debug_logging(
    agent: Agent, 
    query: str, 
    service_type: AgentServiceType, 
    reasoning_mode: bool = False
) -> AsyncGenerator[str, None]:
    """ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ä»˜ãã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ï¼ˆæ¨è«–ãƒ¢ãƒ¼ãƒ‰å¯¾å¿œãƒ»æ”¹å–„ç‰ˆï¼‰"""
    
    try:
        # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹ãªå ´åˆã¯ReasoningToolsã‚’è¿½åŠ 
        if reasoning_mode:
            if hasattr(agent, 'tools') and agent.tools is not None:
                reasoning_tools = ReasoningTools(add_instructions=True)
                agent.tools = agent.tools + [reasoning_tools]
            else:
                agent.tools = [ReasoningTools(add_instructions=True)]
            
            yield f"data: {json.dumps({'type': 'reasoning_mode_active', 'content': 'Reasoning mode activated', 'timestamp': datetime.now().isoformat()})}\n\n"
        
        response = await agent.arun(query, stream=True)
        
        tools_used = []
        debug_info = {"tool_calls": []}
        reasoning_content = ""
        answer_content = ""
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        if hasattr(response, '__aiter__'):
            async for chunk in response:
                # ãƒãƒ£ãƒ³ã‚¯å‡¦ç†ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€ä¿¡ã®ä¸¡æ–¹ã‚’è¡Œã†
                chunk_result = await _process_streaming_chunk(
                    chunk, service_type, tools_used, debug_info, reasoning_content, answer_content, reasoning_mode
                )
                # è¾æ›¸ã‹ã‚‰å€‹åˆ¥ã®å€¤ã‚’å–å¾—
                tools_used = chunk_result['tools_used']
                debug_info = chunk_result['debug_info']  
                reasoning_content = chunk_result['reasoning_content']
                answer_content = chunk_result['answer_content']
                
                # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é€ä¿¡
                streaming_data = chunk_result['streaming_response']
                if streaming_data:
                    yield streaming_data

        # æœ€çµ‚å®Œäº†é€šçŸ¥
        completion_data = {
            "type": "completion",
            "tools_used": tools_used,
            "debug_info": debug_info,
            "reasoning_content": reasoning_content,
            "answer_content": answer_content,
            "total_tools": len(tools_used),
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(completion_data)}\n\n"
        
        logger.info(f"[{service_type}] Stream completed - Tools: {len(tools_used)}, Reasoning: {len(reasoning_content)} chars")
        
    except Exception as e:
        print(f"âŒ [{service_type}] Stream error: {str(e)}")
        traceback.print_exc()
        error_data = {
            "type": "error",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }
        yield f"data: {json.dumps(error_data)}\n\n"
        raise

async def _process_streaming_chunk(chunk, service_type, tools_used, debug_info, reasoning_content, answer_content, reasoning_mode):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã®å‡¦ç†ã¨ãƒãƒ³ã‚¹é€ä¿¡"""
    try:
        streaming_response = ""
        
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—å‡¦ç†
        if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
            for tool_call in chunk.tool_calls:
                tool_name = tool_call.get('name', 'Unknown') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'Unknown')
                tool_args = tool_call.get('arguments', {}) if isinstance(tool_call, dict) else getattr(tool_call, 'arguments', {})
                
                print(f" [{service_type}] Using tool: {tool_name}")
                
                tools_used.append({"name": tool_name, "arguments": tool_args})
                debug_info["tool_calls"].append({"name": tool_name, "arguments": tool_args})

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„å‡¦ç†ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°é€ä¿¡
        if hasattr(chunk, 'content') and chunk.content:
            content = chunk.content
            answer_content += content
            
            if content.strip():
                preview = content[:50].replace('\n', ' ')
                print(f" [{service_type}] Chunk: {preview}{'...' if len(content) > 50 else ''}")
                
                # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã«é€ä¿¡ã™ã‚‹ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿
                chunk_data = {
                    "type": "content_chunk",
                    "content": content,
                    "timestamp": datetime.now().isoformat()
                }
                streaming_response = f"data: {json.dumps(chunk_data)}\n\n"
        
        return {
            'tools_used': tools_used,
            'debug_info': debug_info,
            'reasoning_content': reasoning_content,
            'answer_content': answer_content,
            'streaming_response': streaming_response
        }
        
    except Exception as e:
        logger.error(f"[{service_type}] Error processing chunk: {e}")
        return {
            'tools_used': tools_used,
            'debug_info': debug_info,
            'reasoning_content': reasoning_content,
            'answer_content': answer_content,
            'streaming_response': ""
        }

async def process_chunk(chunk, service_type: AgentServiceType, content_parts: list, tools_used: list, debug_info: dict):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆå¿…è¦æœ€å°é™ã®ãƒ­ã‚°ï¼‰"""
    try:
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ±ã®ãƒ­ã‚°
        if hasattr(chunk, 'tool_calls') and chunk.tool_calls:
            for tool_call in chunk.tool_calls:
                tool_name = tool_call.get('name', 'Unknown') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'Unknown')
                tool_args = tool_call.get('arguments', {}) if isinstance(tool_call, dict) else getattr(tool_call, 'arguments', {})
                
                print(f"ï¿½ï¿½ [{service_type}] Using tool: {tool_name}")
                
                tools_used.append({"name": tool_name, "arguments": tool_args})
                debug_info["tool_calls"].append({"name": tool_name, "arguments": tool_args})
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒ£ãƒ³ã‚¯è¡¨ç¤º
        if hasattr(chunk, 'content') and chunk.content:
            content_parts.append(chunk.content)
            if len(chunk.content) > 0:
                preview = chunk.content[:50].replace('\n', ' ')
                print(f"ï¿½ï¿½ [{service_type}] Chunk: {preview}{'...' if len(chunk.content) > 50 else ''}")
                
    except Exception as e:
        logger.error(f"[{service_type}] Error processing chunk: {e}")

async def process_response(response, service_type: AgentServiceType, content_parts: list, tools_used: list, debug_info: dict):
    """é€šå¸¸ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å‡¦ç†ï¼ˆå¿…è¦æœ€å°é™ã®ãƒ­ã‚°ï¼‰"""
    try:
        # ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—æƒ…å ±ã®å‡¦ç†
        if hasattr(response, 'tool_calls') and response.tool_calls:
            for tool_call in response.tool_calls:
                tool_name = tool_call.get('name', 'Unknown') if isinstance(tool_call, dict) else getattr(tool_call, 'name', 'Unknown')
                tool_args = tool_call.get('arguments', {}) if isinstance(tool_call, dict) else getattr(tool_call, 'arguments', {})
                
                print(f"ğŸ”§ [{service_type}] Using tool: {tool_name}")
                tools_used.append({"name": tool_name, "arguments": tool_args})
                debug_info["tool_calls"].append({"name": tool_name, "arguments": tool_args})
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†
        if hasattr(response, 'content'):
            content_parts.append(str(response.content))
            
    except Exception as e:
        logger.error(f"[{service_type}] Error processing response: {e}")